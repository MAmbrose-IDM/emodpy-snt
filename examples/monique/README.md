# Requirements
This project requires the watchdog package for file system monitoring:
```bash
pip install watchdog

```

# SNT Simulation Launcher

This repository contains a non-blocking experiment submission system for emodpy-snt, designed to manage large-scale simulation workflows efficiently. It includes:

**submitter_parallel.py**: submits one or more experiments to a newly created suite in COMPS based on a scenario file, by launching run_simulations.py as a subprocess for each scenario row.

**run_simulations.py**: runs a single experiment in COMPS and waits for it to finish in the background. Once the experiment is successfully done, it triggers post_ssmt.py as a separate background subprocess.

**post_ssmt.py**: runs the SSMT analyzer in COMPS for the experiment submitted in the previous step.

Analyzer will download needed files to local.

With everything running in the background, each scenario's output is streamed live and written to its own dedicated log file.

Automatic scenario status tracking and optional post-run analyzers (e.g., SSMT or local analysis).

---

##  Usage Overview

### 1. Prepare Your Scenario File

Create a scenario CSV file like: C:\github\emodpy-snt\data\example_files\simulation_inputs\_intervention_file_references\Interventions_to_present.csv

Only rows with `status == "run"` will be launched by the submitter.

---
### 2. Submit Experiments (Non-blocking)

```bash
python submit_parallel.py
```
* Submits each run-marked row from the scenario file. 
* Each submit run only submit experiments in the same scenario file
* Generate a suite to hold all experiments (one row is one experiment) in scenario file 
* Writes logs to logs/scenario_<experiment_type>_<index>.log
* Tracks submitted suite_id in suite_tracking_<experiment_type>.csv
* <experiment_type> is either "to_present" or "future_projections" depending on FUTURE_PROJECTIONS = True

### 3. Use watchdog to Monitor Folder for Analyzer Jobs

```bash
python watcher.py
```
This is another way to run analyzer separately from run_simulation.py with watchdog functionality, so you can:
This script provides an alternative way to run analyzers separately from run_simulation.py, using an event-driven approach powered by watchdog. It enables:

* Controlled execution: limit how many analyzers run concurrently

* Resource safety: prevent CPU or memory exhaustion on local machines

* Clean separation: decouple simulation submission from post-analysis

* Efficient monitoring: respond to file changes without constant polling


### 4. How It Works
 * submit_parallel.py
Creates a new suite for each scenario (or reuses an existing one).

For each scenario row in the input CSV:

A subprocess is launched to run run_simulations.py.

That process submits an experiment to COMPS.

Once the experiment finishes:

The scenario CSV is updated with status "done" for that row.

The post_run() function in run_simulations.py creates a .ready file containing:

Experiment ID, Experiment name, Experiment type

This .ready file is placed in the analyzer_queue/ folder for analysis.

* watcher.py (Runs Separately)
Monitors the analyzer_queue/ folder for files with the .ready extension.

These .ready files are generated by run_simulations.py (via _post_run()).

When a .ready file is detected:

It is renamed to .working

An analyzer subprocess is launched (either post_ssmt.py or post_analysis.py)

Once the analyzer completes:

If successful, the file is renamed to .done

If it fails, the file is renamed to .fail

Analyzer logs are saved to:

Logs for analyzer are streamed to logs/ssmt_<experiment_type>_<experiment_id>.log or logs/local_<experiment_type>_<experiment_id>.log

## CLI Arguments (run_simulations.py)
```bash
python run_simulations.py \
    --suite-id <suite_id> \
    --scen-index <row_index> \
    --scenario-fname <scenario_file> \
    --show-warnings-once True|False|None
```

## CLI Arguments (post_ssmt.py or post_analysis.py)
```bash
python post_ssmt.py \
  --exp_id <exp_id> \
  --type <experiment_type> \
  --name <experiment_name>
```

## Output Files
* logs/scenario_<experiment_type>_<index>.log	Per-scenario log file 
* suite_tracking_<type>.csv	Tracks submitted suite ID and timestamp
* Interventions_*.csv	Scenario inputs, automatically updated with status = queued after submission, then update again to "done" once the experiment is finished
* suite/suite_<id>/exp_<exp_id>.txt	Dump of submitted experiment ID
